{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic data  (methods of handling Imbalanced data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Exploring\n",
    "The data that is used in this project originally comes from the UCI machine learning repository (https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008). The data consists of over 100000 hospital admissions from patients with diabetes from 130 US hospitals between 1999-2008.\n",
    "\n",
    "In this part, We tried to explore the following things:\n",
    "* The columns and records of the data\n",
    "* The data type of each column\n",
    "* Whether there is missing values such as null or NAs\n",
    "* The unique values and their counts of each columns\n",
    "* The ratio between the number of readmitted records and non-readmitted records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101766, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>patient_nbr</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101761</td>\n",
       "      <td>443847548</td>\n",
       "      <td>100162476</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Male</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Down</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101762</td>\n",
       "      <td>443847782</td>\n",
       "      <td>74694222</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101763</td>\n",
       "      <td>443854148</td>\n",
       "      <td>41088789</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Down</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101764</td>\n",
       "      <td>443857166</td>\n",
       "      <td>31693671</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[80-90)</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101765</td>\n",
       "      <td>443867222</td>\n",
       "      <td>175429310</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[70-80)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101766 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        encounter_id  patient_nbr             race  gender      age weight  \\\n",
       "0            2278392      8222157        Caucasian  Female   [0-10)      ?   \n",
       "1             149190     55629189        Caucasian  Female  [10-20)      ?   \n",
       "2              64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n",
       "3             500364     82442376        Caucasian    Male  [30-40)      ?   \n",
       "4              16680     42519267        Caucasian    Male  [40-50)      ?   \n",
       "...              ...          ...              ...     ...      ...    ...   \n",
       "101761     443847548    100162476  AfricanAmerican    Male  [70-80)      ?   \n",
       "101762     443847782     74694222  AfricanAmerican  Female  [80-90)      ?   \n",
       "101763     443854148     41088789        Caucasian    Male  [70-80)      ?   \n",
       "101764     443857166     31693671        Caucasian  Female  [80-90)      ?   \n",
       "101765     443867222    175429310        Caucasian    Male  [70-80)      ?   \n",
       "\n",
       "        admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                       6                        25                    1   \n",
       "1                       1                         1                    7   \n",
       "2                       1                         1                    7   \n",
       "3                       1                         1                    7   \n",
       "4                       1                         1                    7   \n",
       "...                   ...                       ...                  ...   \n",
       "101761                  1                         3                    7   \n",
       "101762                  1                         4                    5   \n",
       "101763                  1                         1                    7   \n",
       "101764                  2                         3                    7   \n",
       "101765                  1                         1                    7   \n",
       "\n",
       "        time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
       "0                      1  ...          No      No                   No   \n",
       "1                      3  ...          No      Up                   No   \n",
       "2                      2  ...          No      No                   No   \n",
       "3                      2  ...          No      Up                   No   \n",
       "4                      1  ...          No  Steady                   No   \n",
       "...                  ...  ...         ...     ...                  ...   \n",
       "101761                 3  ...          No    Down                   No   \n",
       "101762                 5  ...          No  Steady                   No   \n",
       "101763                 1  ...          No    Down                   No   \n",
       "101764                10  ...          No      Up                   No   \n",
       "101765                 6  ...          No      No                   No   \n",
       "\n",
       "        glipizide-metformin  glimepiride-pioglitazone  \\\n",
       "0                        No                        No   \n",
       "1                        No                        No   \n",
       "2                        No                        No   \n",
       "3                        No                        No   \n",
       "4                        No                        No   \n",
       "...                     ...                       ...   \n",
       "101761                   No                        No   \n",
       "101762                   No                        No   \n",
       "101763                   No                        No   \n",
       "101764                   No                        No   \n",
       "101765                   No                        No   \n",
       "\n",
       "        metformin-rosiglitazone  metformin-pioglitazone  change diabetesMed  \\\n",
       "0                            No                      No      No          No   \n",
       "1                            No                      No      Ch         Yes   \n",
       "2                            No                      No      No         Yes   \n",
       "3                            No                      No      Ch         Yes   \n",
       "4                            No                      No      Ch         Yes   \n",
       "...                         ...                     ...     ...         ...   \n",
       "101761                       No                      No      Ch         Yes   \n",
       "101762                       No                      No      No         Yes   \n",
       "101763                       No                      No      Ch         Yes   \n",
       "101764                       No                      No      Ch         Yes   \n",
       "101765                       No                      No      No          No   \n",
       "\n",
       "       readmitted  \n",
       "0              NO  \n",
       "1             >30  \n",
       "2              NO  \n",
       "3              NO  \n",
       "4              NO  \n",
       "...           ...  \n",
       "101761        >30  \n",
       "101762         NO  \n",
       "101763         NO  \n",
       "101764         NO  \n",
       "101765         NO  \n",
       "\n",
       "[101766 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encounter_id                 int64\n",
       "patient_nbr                  int64\n",
       "race                        object\n",
       "gender                      object\n",
       "age                         object\n",
       "weight                      object\n",
       "admission_type_id            int64\n",
       "discharge_disposition_id     int64\n",
       "admission_source_id          int64\n",
       "time_in_hospital             int64\n",
       "payer_code                  object\n",
       "medical_specialty           object\n",
       "num_lab_procedures           int64\n",
       "num_procedures               int64\n",
       "num_medications              int64\n",
       "number_outpatient            int64\n",
       "number_emergency             int64\n",
       "number_inpatient             int64\n",
       "diag_1                      object\n",
       "diag_2                      object\n",
       "diag_3                      object\n",
       "number_diagnoses             int64\n",
       "max_glu_serum               object\n",
       "A1Cresult                   object\n",
       "metformin                   object\n",
       "repaglinide                 object\n",
       "nateglinide                 object\n",
       "chlorpropamide              object\n",
       "glimepiride                 object\n",
       "acetohexamide               object\n",
       "glipizide                   object\n",
       "glyburide                   object\n",
       "tolbutamide                 object\n",
       "pioglitazone                object\n",
       "rosiglitazone               object\n",
       "acarbose                    object\n",
       "miglitol                    object\n",
       "troglitazone                object\n",
       "tolazamide                  object\n",
       "examide                     object\n",
       "citoglipton                 object\n",
       "insulin                     object\n",
       "glyburide-metformin         object\n",
       "glipizide-metformin         object\n",
       "glimepiride-pioglitazone    object\n",
       "metformin-rosiglitazone     object\n",
       "metformin-pioglitazone      object\n",
       "change                      object\n",
       "diabetesMed                 object\n",
       "readmitted                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encounter_id                0\n",
       "patient_nbr                 0\n",
       "race                        0\n",
       "gender                      0\n",
       "age                         0\n",
       "weight                      0\n",
       "admission_type_id           0\n",
       "discharge_disposition_id    0\n",
       "admission_source_id         0\n",
       "time_in_hospital            0\n",
       "payer_code                  0\n",
       "medical_specialty           0\n",
       "num_lab_procedures          0\n",
       "num_procedures              0\n",
       "num_medications             0\n",
       "number_outpatient           0\n",
       "number_emergency            0\n",
       "number_inpatient            0\n",
       "diag_1                      0\n",
       "diag_2                      0\n",
       "diag_3                      0\n",
       "number_diagnoses            0\n",
       "max_glu_serum               0\n",
       "A1Cresult                   0\n",
       "metformin                   0\n",
       "repaglinide                 0\n",
       "nateglinide                 0\n",
       "chlorpropamide              0\n",
       "glimepiride                 0\n",
       "acetohexamide               0\n",
       "glipizide                   0\n",
       "glyburide                   0\n",
       "tolbutamide                 0\n",
       "pioglitazone                0\n",
       "rosiglitazone               0\n",
       "acarbose                    0\n",
       "miglitol                    0\n",
       "troglitazone                0\n",
       "tolazamide                  0\n",
       "examide                     0\n",
       "citoglipton                 0\n",
       "insulin                     0\n",
       "glyburide-metformin         0\n",
       "glipizide-metformin         0\n",
       "glimepiride-pioglitazone    0\n",
       "metformin-rosiglitazone     0\n",
       "metformin-pioglitazone      0\n",
       "change                      0\n",
       "diabetesMed                 0\n",
       "readmitted                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encounter_id                0\n",
       "patient_nbr                 0\n",
       "race                        0\n",
       "gender                      0\n",
       "age                         0\n",
       "weight                      0\n",
       "admission_type_id           0\n",
       "discharge_disposition_id    0\n",
       "admission_source_id         0\n",
       "time_in_hospital            0\n",
       "payer_code                  0\n",
       "medical_specialty           0\n",
       "num_lab_procedures          0\n",
       "num_procedures              0\n",
       "num_medications             0\n",
       "number_outpatient           0\n",
       "number_emergency            0\n",
       "number_inpatient            0\n",
       "diag_1                      0\n",
       "diag_2                      0\n",
       "diag_3                      0\n",
       "number_diagnoses            0\n",
       "max_glu_serum               0\n",
       "A1Cresult                   0\n",
       "metformin                   0\n",
       "repaglinide                 0\n",
       "nateglinide                 0\n",
       "chlorpropamide              0\n",
       "glimepiride                 0\n",
       "acetohexamide               0\n",
       "glipizide                   0\n",
       "glyburide                   0\n",
       "tolbutamide                 0\n",
       "pioglitazone                0\n",
       "rosiglitazone               0\n",
       "acarbose                    0\n",
       "miglitol                    0\n",
       "troglitazone                0\n",
       "tolazamide                  0\n",
       "examide                     0\n",
       "citoglipton                 0\n",
       "insulin                     0\n",
       "glyburide-metformin         0\n",
       "glipizide-metformin         0\n",
       "glimepiride-pioglitazone    0\n",
       "metformin-rosiglitazone     0\n",
       "metformin-pioglitazone      0\n",
       "change                      0\n",
       "diabetesMed                 0\n",
       "readmitted                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96210942     1\n",
      "89943846     1\n",
      "384306986    1\n",
      "94650156     1\n",
      "83156784     1\n",
      "            ..\n",
      "74454612     1\n",
      "208073976    1\n",
      "166229592    1\n",
      "38340702     1\n",
      "77856768     1\n",
      "Name: encounter_id, Length: 101766, dtype: int64\n",
      "-----------------------------------------\n",
      "88785891     40\n",
      "43140906     28\n",
      "23199021     23\n",
      "1660293      23\n",
      "88227540     23\n",
      "             ..\n",
      "71081460      1\n",
      "30060018      1\n",
      "67443444      1\n",
      "141344240     1\n",
      "93251151      1\n",
      "Name: patient_nbr, Length: 71518, dtype: int64\n",
      "-----------------------------------------\n",
      "Caucasian          76099\n",
      "AfricanAmerican    19210\n",
      "?                   2273\n",
      "Hispanic            2037\n",
      "Other               1506\n",
      "Asian                641\n",
      "Name: race, dtype: int64\n",
      "-----------------------------------------\n",
      "Female             54708\n",
      "Male               47055\n",
      "Unknown/Invalid        3\n",
      "Name: gender, dtype: int64\n",
      "-----------------------------------------\n",
      "[70-80)     26068\n",
      "[60-70)     22483\n",
      "[50-60)     17256\n",
      "[80-90)     17197\n",
      "[40-50)      9685\n",
      "[30-40)      3775\n",
      "[90-100)     2793\n",
      "[20-30)      1657\n",
      "[10-20)       691\n",
      "[0-10)        161\n",
      "Name: age, dtype: int64\n",
      "-----------------------------------------\n",
      "?            98569\n",
      "[75-100)      1336\n",
      "[50-75)        897\n",
      "[100-125)      625\n",
      "[125-150)      145\n",
      "[25-50)         97\n",
      "[0-25)          48\n",
      "[150-175)       35\n",
      "[175-200)       11\n",
      ">200             3\n",
      "Name: weight, dtype: int64\n",
      "-----------------------------------------\n",
      "1    53990\n",
      "3    18869\n",
      "2    18480\n",
      "6     5291\n",
      "5     4785\n",
      "8      320\n",
      "7       21\n",
      "4       10\n",
      "Name: admission_type_id, dtype: int64\n",
      "-----------------------------------------\n",
      "1     60234\n",
      "3     13954\n",
      "6     12902\n",
      "18     3691\n",
      "2      2128\n",
      "22     1993\n",
      "11     1642\n",
      "5      1184\n",
      "25      989\n",
      "4       815\n",
      "7       623\n",
      "23      412\n",
      "13      399\n",
      "14      372\n",
      "28      139\n",
      "8       108\n",
      "15       63\n",
      "24       48\n",
      "9        21\n",
      "17       14\n",
      "16       11\n",
      "19        8\n",
      "10        6\n",
      "27        5\n",
      "12        3\n",
      "20        2\n",
      "Name: discharge_disposition_id, dtype: int64\n",
      "-----------------------------------------\n",
      "7     57494\n",
      "1     29565\n",
      "17     6781\n",
      "4      3187\n",
      "6      2264\n",
      "2      1104\n",
      "5       855\n",
      "3       187\n",
      "20      161\n",
      "9       125\n",
      "8        16\n",
      "22       12\n",
      "10        8\n",
      "11        2\n",
      "14        2\n",
      "25        2\n",
      "13        1\n",
      "Name: admission_source_id, dtype: int64\n",
      "-----------------------------------------\n",
      "3     17756\n",
      "2     17224\n",
      "1     14208\n",
      "4     13924\n",
      "5      9966\n",
      "6      7539\n",
      "7      5859\n",
      "8      4391\n",
      "9      3002\n",
      "10     2342\n",
      "11     1855\n",
      "12     1448\n",
      "13     1210\n",
      "14     1042\n",
      "Name: time_in_hospital, dtype: int64\n",
      "-----------------------------------------\n",
      "?     40256\n",
      "MC    32439\n",
      "HM     6274\n",
      "SP     5007\n",
      "BC     4655\n",
      "MD     3532\n",
      "CP     2533\n",
      "UN     2448\n",
      "CM     1937\n",
      "OG     1033\n",
      "PO      592\n",
      "DM      549\n",
      "CH      146\n",
      "WC      135\n",
      "OT       95\n",
      "MP       79\n",
      "SI       55\n",
      "FR        1\n",
      "Name: payer_code, dtype: int64\n",
      "-----------------------------------------\n",
      "?                                   49949\n",
      "InternalMedicine                    14635\n",
      "Emergency/Trauma                     7565\n",
      "Family/GeneralPractice               7440\n",
      "Cardiology                           5352\n",
      "                                    ...  \n",
      "Dermatology                             1\n",
      "Speech                                  1\n",
      "Perinatology                            1\n",
      "SportsMedicine                          1\n",
      "Surgery-PlasticwithinHeadandNeck        1\n",
      "Name: medical_specialty, Length: 73, dtype: int64\n",
      "-----------------------------------------\n",
      "1      3208\n",
      "43     2804\n",
      "44     2496\n",
      "45     2376\n",
      "38     2213\n",
      "       ... \n",
      "107       1\n",
      "118       1\n",
      "129       1\n",
      "120       1\n",
      "121       1\n",
      "Name: num_lab_procedures, Length: 118, dtype: int64\n",
      "-----------------------------------------\n",
      "0    46652\n",
      "1    20742\n",
      "2    12717\n",
      "3     9443\n",
      "6     4954\n",
      "4     4180\n",
      "5     3078\n",
      "Name: num_procedures, dtype: int64\n",
      "-----------------------------------------\n",
      "13    6086\n",
      "12    6004\n",
      "11    5795\n",
      "15    5792\n",
      "14    5707\n",
      "      ... \n",
      "70       2\n",
      "75       2\n",
      "74       1\n",
      "79       1\n",
      "81       1\n",
      "Name: num_medications, Length: 75, dtype: int64\n",
      "-----------------------------------------\n",
      "0     85027\n",
      "1      8547\n",
      "2      3594\n",
      "3      2042\n",
      "4      1099\n",
      "5       533\n",
      "6       303\n",
      "7       155\n",
      "8        98\n",
      "9        83\n",
      "10       57\n",
      "11       42\n",
      "13       31\n",
      "12       30\n",
      "14       28\n",
      "15       20\n",
      "16       15\n",
      "17        8\n",
      "21        7\n",
      "20        7\n",
      "22        5\n",
      "18        5\n",
      "19        3\n",
      "24        3\n",
      "27        3\n",
      "23        2\n",
      "25        2\n",
      "26        2\n",
      "29        2\n",
      "33        2\n",
      "35        2\n",
      "36        2\n",
      "40        1\n",
      "28        1\n",
      "34        1\n",
      "37        1\n",
      "38        1\n",
      "39        1\n",
      "42        1\n",
      "Name: number_outpatient, dtype: int64\n",
      "-----------------------------------------\n",
      "0     90383\n",
      "1      7677\n",
      "2      2042\n",
      "3       725\n",
      "4       374\n",
      "5       192\n",
      "6        94\n",
      "7        73\n",
      "8        50\n",
      "10       34\n",
      "9        33\n",
      "11       23\n",
      "13       12\n",
      "12       10\n",
      "22        6\n",
      "18        5\n",
      "16        5\n",
      "19        4\n",
      "20        4\n",
      "14        3\n",
      "15        3\n",
      "21        2\n",
      "25        2\n",
      "76        1\n",
      "54        1\n",
      "24        1\n",
      "28        1\n",
      "29        1\n",
      "37        1\n",
      "42        1\n",
      "46        1\n",
      "64        1\n",
      "63        1\n",
      "Name: number_emergency, dtype: int64\n",
      "-----------------------------------------\n",
      "0     67630\n",
      "1     19521\n",
      "2      7566\n",
      "3      3411\n",
      "4      1622\n",
      "5       812\n",
      "6       480\n",
      "7       268\n",
      "8       151\n",
      "9       111\n",
      "10       61\n",
      "11       49\n",
      "12       34\n",
      "13       20\n",
      "14       10\n",
      "15        9\n",
      "16        6\n",
      "19        2\n",
      "17        1\n",
      "18        1\n",
      "21        1\n",
      "Name: number_inpatient, dtype: int64\n",
      "-----------------------------------------\n",
      "428    6862\n",
      "414    6581\n",
      "786    4016\n",
      "410    3614\n",
      "486    3508\n",
      "       ... \n",
      "976       1\n",
      "827       1\n",
      "366       1\n",
      "838       1\n",
      "974       1\n",
      "Name: diag_1, Length: 717, dtype: int64\n",
      "-----------------------------------------\n",
      "276     6752\n",
      "428     6662\n",
      "250     6071\n",
      "427     5036\n",
      "401     3736\n",
      "        ... \n",
      "605        1\n",
      "374        1\n",
      "350        1\n",
      "5          1\n",
      "E813       1\n",
      "Name: diag_2, Length: 749, dtype: int64\n",
      "-----------------------------------------\n",
      "250     11555\n",
      "401      8289\n",
      "276      5175\n",
      "428      4577\n",
      "427      3955\n",
      "        ...  \n",
      "361         1\n",
      "265         1\n",
      "385         1\n",
      "217         1\n",
      "E892        1\n",
      "Name: diag_3, Length: 790, dtype: int64\n",
      "-----------------------------------------\n",
      "9     49474\n",
      "5     11393\n",
      "8     10616\n",
      "7     10393\n",
      "6     10161\n",
      "4      5537\n",
      "3      2835\n",
      "2      1023\n",
      "1       219\n",
      "16       45\n",
      "10       17\n",
      "13       16\n",
      "11       11\n",
      "15       10\n",
      "12        9\n",
      "14        7\n",
      "Name: number_diagnoses, dtype: int64\n",
      "-----------------------------------------\n",
      "None    96420\n",
      "Norm     2597\n",
      ">200     1485\n",
      ">300     1264\n",
      "Name: max_glu_serum, dtype: int64\n",
      "-----------------------------------------\n",
      "None    84748\n",
      ">8       8216\n",
      "Norm     4990\n",
      ">7       3812\n",
      "Name: A1Cresult, dtype: int64\n",
      "-----------------------------------------\n",
      "No        81778\n",
      "Steady    18346\n",
      "Up         1067\n",
      "Down        575\n",
      "Name: metformin, dtype: int64\n",
      "-----------------------------------------\n",
      "No        100227\n",
      "Steady      1384\n",
      "Up           110\n",
      "Down          45\n",
      "Name: repaglinide, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101063\n",
      "Steady       668\n",
      "Up            24\n",
      "Down          11\n",
      "Name: nateglinide, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101680\n",
      "Steady        79\n",
      "Up             6\n",
      "Down           1\n",
      "Name: chlorpropamide, dtype: int64\n",
      "-----------------------------------------\n",
      "No        96575\n",
      "Steady     4670\n",
      "Up          327\n",
      "Down        194\n",
      "Name: glimepiride, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101765\n",
      "Steady         1\n",
      "Name: acetohexamide, dtype: int64\n",
      "-----------------------------------------\n",
      "No        89080\n",
      "Steady    11356\n",
      "Up          770\n",
      "Down        560\n",
      "Name: glipizide, dtype: int64\n",
      "-----------------------------------------\n",
      "No        91116\n",
      "Steady     9274\n",
      "Up          812\n",
      "Down        564\n",
      "Name: glyburide, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101743\n",
      "Steady        23\n",
      "Name: tolbutamide, dtype: int64\n",
      "-----------------------------------------\n",
      "No        94438\n",
      "Steady     6976\n",
      "Up          234\n",
      "Down        118\n",
      "Name: pioglitazone, dtype: int64\n",
      "-----------------------------------------\n",
      "No        95401\n",
      "Steady     6100\n",
      "Up          178\n",
      "Down         87\n",
      "Name: rosiglitazone, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101458\n",
      "Steady       295\n",
      "Up            10\n",
      "Down           3\n",
      "Name: acarbose, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101728\n",
      "Steady        31\n",
      "Down           5\n",
      "Up             2\n",
      "Name: miglitol, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101763\n",
      "Steady         3\n",
      "Name: troglitazone, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101727\n",
      "Steady        38\n",
      "Up             1\n",
      "Name: tolazamide, dtype: int64\n",
      "-----------------------------------------\n",
      "No    101766\n",
      "Name: examide, dtype: int64\n",
      "-----------------------------------------\n",
      "No    101766\n",
      "Name: citoglipton, dtype: int64\n",
      "-----------------------------------------\n",
      "No        47383\n",
      "Steady    30849\n",
      "Down      12218\n",
      "Up        11316\n",
      "Name: insulin, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101060\n",
      "Steady       692\n",
      "Up             8\n",
      "Down           6\n",
      "Name: glyburide-metformin, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101753\n",
      "Steady        13\n",
      "Name: glipizide-metformin, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101765\n",
      "Steady         1\n",
      "Name: glimepiride-pioglitazone, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101764\n",
      "Steady         2\n",
      "Name: metformin-rosiglitazone, dtype: int64\n",
      "-----------------------------------------\n",
      "No        101765\n",
      "Steady         1\n",
      "Name: metformin-pioglitazone, dtype: int64\n",
      "-----------------------------------------\n",
      "No    54755\n",
      "Ch    47011\n",
      "Name: change, dtype: int64\n",
      "-----------------------------------------\n",
      "Yes    78363\n",
      "No     23403\n",
      "Name: diabetesMed, dtype: int64\n",
      "-----------------------------------------\n",
      "NO     54864\n",
      ">30    35545\n",
      "<30    11357\n",
      "Name: readmitted, dtype: int64\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in data.columns:\n",
    "    print(data[i].value_counts())\n",
    "    print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values or NAs but lots \"?\" and little \"unknown\" in the data. \n",
    "* race: ?(2273)   \n",
    "* gender: Unknown/Invalid(3)\n",
    "* weight: ?(98569)\n",
    "* payer_code: ?(40256)  \n",
    "* medical_specialty: ?(49949)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1256180247541727"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data['readmitted']=='<30'])/len(data[data['readmitted']!='<30'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Cleaning\n",
    "In this section, we drop some non-influencial columns and some sepecial records, deal with missing values and transform the format or data type of some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaning = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Drop non-influence columns\n",
    "Drop columns that have same value with every records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examide and citoglipton only have 1 value, have no influence to output, drop\n",
    "df_cleaning.drop([\"examide\", \"citoglipton\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Drop special records\n",
    "Drop records according to the value of discharge_disposition_id, which tells us where the patient went after the hospitalization. If we look at the IDs_mapping.csv we can see that 11,13,14,19,20,21 are related to death or hospice. We should remove these samples from the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discharge_disposition_id\n",
       "1     60234\n",
       "2      2128\n",
       "3     13954\n",
       "4       815\n",
       "5      1184\n",
       "6     12902\n",
       "7       623\n",
       "8       108\n",
       "9        21\n",
       "10        6\n",
       "11     1642\n",
       "12        3\n",
       "13      399\n",
       "14      372\n",
       "15       63\n",
       "16       11\n",
       "17       14\n",
       "18     3691\n",
       "19        8\n",
       "20        2\n",
       "22     1993\n",
       "23      412\n",
       "24       48\n",
       "25      989\n",
       "27        5\n",
       "28      139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaning.groupby('discharge_disposition_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaning = df_cleaning.loc[~data.discharge_disposition_id.isin([11,13,14,19,20,21])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Deal with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with weight missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weight\n",
       "?            96218\n",
       "[75-100)      1312\n",
       "[50-75)        867\n",
       "[100-125)      617\n",
       "[125-150)      143\n",
       "[25-50)         90\n",
       "[0-25)          48\n",
       "[150-175)       34\n",
       "[175-200)       11\n",
       ">200             3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaning.groupby('weight').size().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too much ? in weight, and other features have no help to fill in the missing value in weight,\n",
    "# so transform the weight feature into binary.\n",
    "df_cleaning.weight[df_cleaning['weight']!='?'] = \"1\"\n",
    "df_cleaning.weight[df_cleaning['weight']=='?'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop records with \"unknown\" value in gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little unknown in gender, so just drop\n",
    "df_cleaning = df_cleaning[df_cleaning['gender']!= 'Unknown/Invalid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regard \"?\" in other columns as a kind of category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Change the data type and format of some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type of ids\n",
    "ids_col = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "df_cleaning[ids_col] = df_cleaning[ids_col].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map age\n",
    "age_map = {'[0-10)':5, \n",
    "          '[10-20)':15, \n",
    "          '[20-30)':25, \n",
    "          '[30-40)':35, \n",
    "          '[40-50)':45, \n",
    "          '[50-60)':55,\n",
    "          '[60-70)':65, \n",
    "          '[70-80)':75, \n",
    "          '[80-90)':85, \n",
    "          '[90-100)':95}\n",
    "df_cleaning['age'] = df_cleaning['age'].replace(age_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with output values\n",
    "df_cleaning.readmitted[df_cleaning['readmitted'] != '<30'] = 0\n",
    "df_cleaning.readmitted[df_cleaning['readmitted'] == '<30'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Check the data types after cleanig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encounter_id                 int64\n",
       "patient_nbr                  int64\n",
       "race                        object\n",
       "gender                      object\n",
       "age                          int64\n",
       "weight                      object\n",
       "admission_type_id           object\n",
       "discharge_disposition_id    object\n",
       "admission_source_id         object\n",
       "time_in_hospital             int64\n",
       "payer_code                  object\n",
       "medical_specialty           object\n",
       "num_lab_procedures           int64\n",
       "num_procedures               int64\n",
       "num_medications              int64\n",
       "number_outpatient            int64\n",
       "number_emergency             int64\n",
       "number_inpatient             int64\n",
       "diag_1                      object\n",
       "diag_2                      object\n",
       "diag_3                      object\n",
       "number_diagnoses             int64\n",
       "max_glu_serum               object\n",
       "A1Cresult                   object\n",
       "metformin                   object\n",
       "repaglinide                 object\n",
       "nateglinide                 object\n",
       "chlorpropamide              object\n",
       "glimepiride                 object\n",
       "acetohexamide               object\n",
       "glipizide                   object\n",
       "glyburide                   object\n",
       "tolbutamide                 object\n",
       "pioglitazone                object\n",
       "rosiglitazone               object\n",
       "acarbose                    object\n",
       "miglitol                    object\n",
       "troglitazone                object\n",
       "tolazamide                  object\n",
       "insulin                     object\n",
       "glyburide-metformin         object\n",
       "glipizide-metformin         object\n",
       "glimepiride-pioglitazone    object\n",
       "metformin-rosiglitazone     object\n",
       "metformin-pioglitazone      object\n",
       "change                      object\n",
       "diabetesMed                 object\n",
       "readmitted                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaning.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Engeering\n",
    "In this section, we create features for our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_cleaning.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['encounter_id', 'patient_nbr', 'race', 'gender', 'age', 'weight',\n",
       "       'admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n",
       "       'time_in_hospital', 'payer_code', 'medical_specialty',\n",
       "       'num_lab_procedures', 'num_procedures', 'num_medications',\n",
       "       'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1',\n",
       "       'diag_2', 'diag_3', 'number_diagnoses', 'max_glu_serum', 'A1Cresult',\n",
       "       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
       "       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
       "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
       "       'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
       "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
       "       'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn these non-numerical data into variables, the simplest thing is to use a technique called one-hot encoding, which will be explained below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical_specialty\n",
       "?                                48614\n",
       "InternalMedicine                 14237\n",
       "Emergency/Trauma                  7419\n",
       "Family/GeneralPractice            7252\n",
       "Cardiology                        5278\n",
       "                                 ...  \n",
       "Speech                               1\n",
       "Pediatrics-InfectiousDiseases        1\n",
       "SportsMedicine                       1\n",
       "Proctology                           1\n",
       "Neurophysiology                      1\n",
       "Length: 73, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature.groupby('medical_specialty').size().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of them are unknown and that the count drops off pretty quickly. We don't want to add 73 new variables since some of them only have a few samples. As an alternative, we can create a new variable that only has 11 options (the top 10 specialities and then an other category). Obviously, there are other options for bucketing, but this is one of the easiest methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = ['?','InternalMedicine','Emergency/Trauma','Family/GeneralPractice', 'Cardiology','Surgery-General' ,\n",
    "          'Nephrology','Orthopedics', 'Orthopedics-Reconstructive','Radiologist']\n",
    "\n",
    "# make a new column with duplicated data\n",
    "df_feature['med_spec'] = df_feature['medical_specialty'].copy()\n",
    "\n",
    "# replace all specialties not in top 10 with 'Other' category\n",
    "df_feature.loc[~df_feature.med_spec.isin(top_10),'med_spec'] = 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In one-hot encoding, you create a new column for each unique value in that column. Then the value of the column is 1 if the sample has that unique value or 0 otherwise. For example, for the column race, we would create new columns ('race_Caucasian','race_AfricanAmerican', etc). If the patient's race is Caucasian, the patient gets a 1 under 'race_Caucasian' and 0 under the rest of the race columns. To create these one-hot encoding columns, we can use the get_dummies function. Now the problem is that if we create a column for each unique value, we have correlated columns. In other words, the value in one column can be figured out by looking at the rest of the columns. For example, if the sample is not AfricanAmerican, Asian, Causasian, Hispance or Other, it must be \"?\". To deal with this, we can use the drop_first option, which will drop the first categorical value for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split column names into numerical and category, in order to get_dummies of category features.\n",
    "numerical = ['age', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', \n",
    "            'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
    "\n",
    "category = ['race', 'gender', 'weight','admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n",
    "       'max_glu_serum', 'A1Cresult', 'metformin','repaglinide',\n",
    "        'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'insulin',\n",
    "        'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "       'metformin-pioglitazone', 'change', 'diabetesMed','payer_code']\n",
    "# We don't include diag1, diag2, diag3 - which are categorical and have a lot of values. \n",
    "# We will not use these as part of this project, but could group these ICD codes to reduce the dimension. \n",
    "# We will use number_diagnoses to capture some of this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99340, 133)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcate = pd.get_dummies(df_feature[category + ['med_spec']],drop_first = True)\n",
    "dfcate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnum = df_feature[numerical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_all = pd.concat([dfnum,dfcate], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Split and Balance the data\n",
    "Imbalanced data refers to a situation where the number of observations is not the same for all the classes in a classification dataset. Most machine learning algorithms work best when the number of samples in each class are about equal. Given that these algorithms aim to minimize the overall error rate, instead of paying special attention to the minority class, they may fail to make an accurate prediction for this class if they donâ€™t get the necessary amount of information about it. Letâ€™s consider an even more extreme example than our breast cancer dataset: assume we had 10 malignant vs 90 benign samples. A machine learning model that has been trained and tested on such a dataset could now predict â€œbenignâ€ for all samples and still gain a very high accuracy. An unbalanced dataset will bias the prediction model towards the more common class! Thus, we need to balance the data and use the right evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X_all.copy()\n",
    "X_columns = X.columns\n",
    "y = df_feature['readmitted'].astype('int')\n",
    "df_data = pd.concat([X,y], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prevalence(y_actual):\n",
    "    return (sum(y_actual)/len(y_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1138916851218039"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevalence before split\n",
    "calc_prevalence(df_data['readmitted'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will split into 80% train, 20% test.  We can use \"sample\" to extract 20% (using \"frac\") of the data to be used for test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prevalence(n = 19868):0.114\n",
      "Train all prevalence(n = 79472):0.114\n"
     ]
    }
   ],
   "source": [
    "# shuffle the samples\n",
    "df_data = df_data.sample(n = len(df_data), random_state = 42)\n",
    "df_data = df_data.reset_index(drop = True)\n",
    "\n",
    "# Save 20% of the data as test data \n",
    "df_test = df_data.sample(frac = 0.2,random_state = 42)\n",
    "\n",
    "# use the rest of the data as training data\n",
    "df_train = df_data.drop(df_test.index)\n",
    "df_train = df_train.sample(n = len(df_train), random_state = 42).reset_index(drop = True)\n",
    "\n",
    "# check the prevalence of train and test\n",
    "print('Test prevalence(n = %d):%.3f'%(len(df_test),calc_prevalence(df_test.readmitted.values)))\n",
    "print('Train all prevalence(n = %d):%.3f'%(len(df_train), calc_prevalence(df_train.readmitted.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Balance data\n",
    "We only balance the train data and keep the test untouched to ensure that the test set reflects the reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Under-sampling\n",
    "Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sample(df_train):\n",
    "    rare_df = df_train[df_train['readmitted']== 1]\n",
    "    abundant_df = df_train.drop(rare_df.index)\n",
    "    under_df = abundant_df.sample(n = len(rare_df), random_state = 1)\n",
    "    df_train_under = pd.concat([rare_df,under_df], axis = 0)\n",
    "    return df_train_under"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Over-sampling\n",
    "On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. Rather than getting rid of abundant samples, new rare samples are generated by using e.g. bootstrapping or SMOTE (Synthetic Minority Over-Sampling Technique) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) bootstrapping\n",
    "If we re-sample records with replacement from our data, we can treat the re-sampled dataset as a new dataset we collected in a parallel universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping(df_train):\n",
    "    abundant_df = df_train[df_train['readmitted']== 0]\n",
    "    rare_df = df_train[df_train['readmitted']== 1]\n",
    "    temp_rare = rare_df.sample(n = len(abundant_df), random_state = 1, replace=True)\n",
    "    df_train_bootstrapping = pd.concat([temp_rare,abundant_df], axis = 0)\n",
    "    return df_train_bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) SMOTE (Synthetic Minority Over-Sampling Technique)\n",
    "This technique was described by Nitesh Chawla, et al. in their 2002 paper named for the technique titled â€œSMOTE: Synthetic Minority Over-sampling Technique.â€\n",
    "\n",
    "SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n",
    "\n",
    "Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(df_train):\n",
    "    smo = SMOTE(random_state=42)\n",
    "    X = df_train[X_columns]\n",
    "    y = df_train[\"readmitted\"]\n",
    "    X_smo, y_smo = smo.fit_sample(X, y)\n",
    "    df_train_smote = pd.concat([X_smo, y_smo], axis = 1)\n",
    "    return df_train_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Ensemble different resampled datasets\n",
    "The easiest way to successfully generalize a model is by using more data. The problem is that out-of-the-box classifiers like logistic regression or random forest tend to generalize by discarding the rare class. One easy best practice is building n models that use all the samples of the rare class and n-differing samples of the abundant class. Given that you want to ensemble 10 models, you would keep e.g. the 1,000 cases of the rare class and randomly sample 10,000 cases of the abundant class. Then you just split the 10,000 cases in 10 chunks and train 10 different models. This approach is simple and perfectly horizontally scalable if you have a lot of data, since you can just train and run your models on different cluster nodes. Ensemble models also tend to generalize better, which makes this approach easy to handle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(df_train):\n",
    "    abundant_df = df_train[df_train['readmitted']== 0]\n",
    "    rare_df = df_train[df_train['readmitted']== 1]\n",
    "    abundant_df = abundant_df.sample(n = len(abundant_df), random_state = 42)    # shuffle the data\n",
    "    abundant_df = abundant_df.reset_index(drop = True)\n",
    "    n = len(abundant_df)//5\n",
    "    df_train_ensemble1 = pd.concat([abundant_df[:n], rare_df], axis = 0)\n",
    "    df_train_ensemble2 = pd.concat([abundant_df[n:(n*2)], rare_df], axis = 0)\n",
    "    df_train_ensemble3 = pd.concat([abundant_df[(n*2):(n*3)], rare_df], axis = 0)\n",
    "    df_train_ensemble4 = pd.concat([abundant_df[(n*3):(n*4)], rare_df], axis = 0)\n",
    "    df_train_ensemble5 = pd.concat([abundant_df[(n*4):], rare_df], axis = 0)\n",
    "    return [df_train_ensemble1, df_train_ensemble2, df_train_ensemble3, df_train_ensemble4, df_train_ensemble5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = ensemble(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Algorithms\n",
    " We use cross validation in train set to tune the parameters and select models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_actual, y_pred, thresh):    \n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    cm = confusion_matrix(y_actual, (y_pred > thresh))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_actual, (y_pred > thresh)).ravel()\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "    TPR = tp/(tp+fn)\n",
    "    FNR = fn/(tp+fn)\n",
    "    f1 = (2*recall*precision)/(recall+precision)\n",
    "    return auc, accuracy, TPR, FNR, recall, precision, specificity, calc_prevalence(y_actual), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "def scale_data(X_train,X_test):\n",
    "    scaler  = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_tf = scaler.transform(X_train)\n",
    "    X_test_tf = scaler.transform(X_test)\n",
    "    return X_train_tf,X_test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "def cross_val(df_train, name, clf, balance_method):\n",
    "    sfolder = StratifiedKFold(n_splits = 5,random_state =1,shuffle=False)\n",
    "    all_columns = df_train.columns\n",
    "    X = df_train[X_columns].values\n",
    "    y = df_train[\"readmitted\"].values\n",
    "    accuracy_list, auc_list, TPR_list, FNR_list, recall_list, precision_list, specificity_list, prevalence_list, f1_list  = [], [], [], [], [], [], [], [], []\n",
    "    for train_index, test_index in sfolder.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        cv_train = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis = 1)\n",
    "        cv_train.columns = all_columns\n",
    "        \n",
    "        # Balance data\n",
    "        if balance_method == \"under_sample\":\n",
    "            cv_train_B = under_sample(cv_train) \n",
    "        if balance_method == \"bootstrapping\":\n",
    "            cv_train_B = bootstrapping(cv_train)\n",
    "        if balance_method == \"SMOTE\":\n",
    "            cv_train_B = smote(cv_train)\n",
    "        \n",
    "        # scale data\n",
    "        X_train_tf, X_test_tf = scale_data(cv_train_B[X_columns].values,X_test)\n",
    "        y_train_B = cv_train_B[\"readmitted\"]\n",
    "        \n",
    "        # fit model\n",
    "        clf.fit(X_train_tf, y_train_B)\n",
    "        y_test_preds = clf.predict_proba(X_test_tf)[:,1]\n",
    "        auc, accuracy, TPR, FNR, recall, precision, specificity, prevalence, f1 = report(y_test, y_test_preds, thresh)\n",
    "        accuracy_list.append(accuracy)\n",
    "        auc_list.append(auc)\n",
    "        TPR_list.append(TPR)\n",
    "        FNR_list.append(FNR)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        specificity_list.append(specificity)\n",
    "        prevalence_list.append(prevalence)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    accuracy_mean, accuracy_std = np.mean(accuracy_list), np.std(accuracy_list)\n",
    "    auc_mean, auc_std = np.mean(auc_list), np.std(auc_list)\n",
    "    TPR_mean, TPR_std = np.mean(TPR_list), np.std(TPR_list)\n",
    "    FNR_mean, FNR_std = np.mean(FNR_list), np.std(FNR_list)\n",
    "    recall_mean, precision_mean = np.mean(recall_list), np.mean(precision_list)\n",
    "    specificity_mean, prevalence_mean, f1_mean = np.mean(specificity_list), np.mean(prevalence_list), np.mean(f1_list)\n",
    "    \n",
    "    print('name:', name) \n",
    "    print('auc_mean:%.3f'%auc_mean, 'auc_std:%.3f'%auc_std, 'accuracy_mean:%.3f'%accuracy_mean, 'accuracy_std:%.3f'%accuracy_std)\n",
    "    print('TPR_mean:%.3f'%TPR_mean, 'TPR_std:%.3f'%TPR_std, 'FNR_mean:%.3f'%FNR_mean, 'FNR_std:%.3f'%FNR_std)\n",
    "    print('recall_mean:%.3f'%recall_mean)\n",
    "    print('precision_mean:%.3f'%precision_mean)\n",
    "    print('specificity_mean:%.3f'%specificity_mean)\n",
    "    print('prevalence_mean:%.3f'%prevalence_mean)\n",
    "    print('f1_mean:%.3f'%f1_mean)\n",
    "    print('  ')\n",
    "    return accuracy_mean, accuracy_std, auc_mean, auc_std, TPR_mean, TPR_std, FNR_mean, FNR_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(clf_list,data_list,X_test,y_test):\n",
    "    accuracy_list, auc_list, TPR_list, FNR_list, recall_list, precision_list, specificity_list, prevalence_list, f1_list  = [], [], [], [], [], [], [], [], []\n",
    "    for i in range(len(data_list)):\n",
    "        data = data_list[i]\n",
    "        X_train_tf, X_test_tf = scale_data(data[X_columns].values,X_test)\n",
    "        y_train = data[\"readmitted\"].values\n",
    "        clf = clf_list[i]\n",
    "        clf.fit(X_train_tf, y_train)\n",
    "        y_test_preds = clf.predict_proba(X_test_tf)[:,1]\n",
    "        auc, accuracy, TPR, FNR, recall, precision, specificity, prevalence, f1 = report(y_test, y_test_preds, thresh)\n",
    "        accuracy_list.append(accuracy)\n",
    "        auc_list.append(auc)\n",
    "        TPR_list.append(TPR)\n",
    "        FNR_list.append(FNR)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        specificity_list.append(specificity)\n",
    "        prevalence_list.append(prevalence)\n",
    "        f1_list.append(f1)\n",
    "    print('auc_mean:%.3f'%np.mean(auc_list), 'accuracy_mean:%.3f'%np.mean(accuracy_list))\n",
    "    print('TPR_mean:%.3f'%np.mean(TPR_list), 'FNR_mean:%.3f'%np.mean(FNR_list))\n",
    "    print('recall_mean:%.3f'%np.mean(recall_list))\n",
    "    print('precision_mean:%.3f'%np.mean(precision_list))\n",
    "    print('specificity_mean:%.3f'%np.mean(specificity_list))\n",
    "    print('prevalence_mean:%.3f'%np.mean(prevalence_list))\n",
    "    print('f1_mean:%.3f'%np.mean(f1_list))\n",
    "    print('  ')\n",
    "    return np.mean(auc_list),np.mean(accuracy_list),np.mean(TPR_list),np.mean(FNR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 10, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 6, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc = SGDClassifier(loss = 'log',alpha = 0.1,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance method: under_sample\n",
      " \n",
      "name: Logistic Regression\n",
      "auc_mean:0.656 auc_std:0.005 accuracy_mean:0.662 accuracy_std:0.005\n",
      "TPR_mean:0.540 TPR_std:0.006 FNR_mean:0.460 FNR_std:0.006\n",
      "recall_mean:0.540\n",
      "precision_mean:0.177\n",
      "specificity_mean:0.678\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.266\n",
      "  \n",
      "name: Decision Tree\n",
      "auc_mean:0.622 auc_std:0.004 accuracy_mean:0.631 accuracy_std:0.009\n",
      "TPR_mean:0.558 TPR_std:0.015 FNR_mean:0.442 FNR_std:0.015\n",
      "recall_mean:0.558\n",
      "precision_mean:0.166\n",
      "specificity_mean:0.637\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.256\n",
      "  \n",
      "name: Random Forest\n",
      "auc_mean:0.655 auc_std:0.005 accuracy_mean:0.628 accuracy_std:0.014\n",
      "TPR_mean:0.593 TPR_std:0.013 FNR_mean:0.407 FNR_std:0.013\n",
      "recall_mean:0.593\n",
      "precision_mean:0.172\n",
      "specificity_mean:0.633\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.267\n",
      "  \n",
      "name: SGDClassifier\n",
      "auc_mean:0.654 auc_std:0.006 accuracy_mean:0.654 accuracy_std:0.007\n",
      "TPR_mean:0.549 TPR_std:0.008 FNR_mean:0.451 FNR_std:0.008\n",
      "recall_mean:0.549\n",
      "precision_mean:0.175\n",
      "specificity_mean:0.668\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.266\n",
      "  \n",
      "name: GradientBoostingClassifier\n",
      "auc_mean:0.624 auc_std:0.002 accuracy_mean:0.605 accuracy_std:0.004\n",
      "TPR_mean:0.570 TPR_std:0.003 FNR_mean:0.430 FNR_std:0.003\n",
      "recall_mean:0.570\n",
      "precision_mean:0.158\n",
      "specificity_mean:0.609\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.247\n",
      "  \n",
      "The average auc of under_sample : 0.6422477177428512\n",
      "--------------------------------------------------------\n",
      "Balance method: bootstrapping\n",
      " \n",
      "name: Logistic Regression\n",
      "auc_mean:0.659 auc_std:0.006 accuracy_mean:0.667 accuracy_std:0.004\n",
      "TPR_mean:0.538 TPR_std:0.005 FNR_mean:0.462 FNR_std:0.005\n",
      "recall_mean:0.538\n",
      "precision_mean:0.179\n",
      "specificity_mean:0.684\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.269\n",
      "  \n",
      "name: Decision Tree\n",
      "auc_mean:0.629 auc_std:0.004 accuracy_mean:0.644 accuracy_std:0.027\n",
      "TPR_mean:0.541 TPR_std:0.035 FNR_mean:0.459 FNR_std:0.035\n",
      "recall_mean:0.541\n",
      "precision_mean:0.169\n",
      "specificity_mean:0.657\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.257\n",
      "  \n",
      "name: Random Forest\n",
      "auc_mean:0.655 auc_std:0.005 accuracy_mean:0.637 accuracy_std:0.011\n",
      "TPR_mean:0.580 TPR_std:0.014 FNR_mean:0.420 FNR_std:0.014\n",
      "recall_mean:0.580\n",
      "precision_mean:0.173\n",
      "specificity_mean:0.644\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.266\n",
      "  \n",
      "name: SGDClassifier\n",
      "auc_mean:0.657 auc_std:0.006 accuracy_mean:0.659 accuracy_std:0.005\n",
      "TPR_mean:0.549 TPR_std:0.005 FNR_mean:0.451 FNR_std:0.005\n",
      "recall_mean:0.549\n",
      "precision_mean:0.177\n",
      "specificity_mean:0.673\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.268\n",
      "  \n",
      "name: GradientBoostingClassifier\n",
      "auc_mean:0.633 auc_std:0.002 accuracy_mean:0.653 accuracy_std:0.003\n",
      "TPR_mean:0.533 TPR_std:0.011 FNR_mean:0.467 FNR_std:0.011\n",
      "recall_mean:0.533\n",
      "precision_mean:0.171\n",
      "specificity_mean:0.668\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.259\n",
      "  \n",
      "The average auc of bootstrapping : 0.6464551334543329\n",
      "--------------------------------------------------------\n",
      "Balance method: SMOTE\n",
      " \n",
      "name: Logistic Regression\n",
      "auc_mean:0.570 auc_std:0.011 accuracy_mean:0.849 accuracy_std:0.002\n",
      "TPR_mean:0.102 TPR_std:0.007 FNR_mean:0.898 FNR_std:0.007\n",
      "recall_mean:0.102\n",
      "precision_mean:0.193\n",
      "specificity_mean:0.945\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.133\n",
      "  \n",
      "name: Decision Tree\n",
      "auc_mean:0.550 auc_std:0.010 accuracy_mean:0.809 accuracy_std:0.008\n",
      "TPR_mean:0.140 TPR_std:0.010 FNR_mean:0.860 FNR_std:0.010\n",
      "recall_mean:0.140\n",
      "precision_mean:0.147\n",
      "specificity_mean:0.894\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.143\n",
      "  \n",
      "name: Random Forest\n",
      "auc_mean:0.541 auc_std:0.006 accuracy_mean:0.791 accuracy_std:0.007\n",
      "TPR_mean:0.167 TPR_std:0.010 FNR_mean:0.833 FNR_std:0.010\n",
      "recall_mean:0.167\n",
      "precision_mean:0.143\n",
      "specificity_mean:0.872\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.154\n",
      "  \n",
      "name: SGDClassifier\n",
      "auc_mean:0.550 auc_std:0.009 accuracy_mean:0.838 accuracy_std:0.003\n",
      "TPR_mean:0.107 TPR_std:0.005 FNR_mean:0.893 FNR_std:0.005\n",
      "recall_mean:0.107\n",
      "precision_mean:0.167\n",
      "specificity_mean:0.932\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.131\n",
      "  \n",
      "name: GradientBoostingClassifier\n",
      "auc_mean:0.587 auc_std:0.007 accuracy_mean:0.848 accuracy_std:0.002\n",
      "TPR_mean:0.104 TPR_std:0.006 FNR_mean:0.896 FNR_std:0.006\n",
      "recall_mean:0.104\n",
      "precision_mean:0.191\n",
      "specificity_mean:0.943\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.135\n",
      "  \n",
      "The average auc of SMOTE : 0.5598292550305086\n",
      "--------------------------------------------------------\n",
      "Balance method: Ensemble different models\n",
      "auc_mean:0.649 accuracy_mean:0.819\n",
      "TPR_mean:0.256 FNR_mean:0.744\n",
      "recall_mean:0.256\n",
      "precision_mean:0.269\n",
      "specificity_mean:0.891\n",
      "prevalence_mean:0.114\n",
      "f1_mean:0.230\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Generate a file of balance methods comparion while selecting best-performenced model\n",
    "with open('Balance methods comparison of Imbalance data.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(['methods','','Logistic Regression','','','','Decision Tree','','','','Random Forest','','','','SGDClassifier','','','','GradientBoostingClassifier','','',''])\n",
    "    names = [\"under_sample\", \"bootstrapping\", \"SMOTE\"]\n",
    "    for i in range(len(names)):\n",
    "        print(\"Balance method:\", names[i])\n",
    "        print(\" \")\n",
    "        lr_accuracy_mean, lr_accuracy_std, lr_auc_mean, lr_auc_std, lr_TPR_mean, lr_TPR_std, lr_FNR_mean, lr_FNR_std = cross_val(df_train, 'Logistic Regression', lr, names[i])        \n",
    "        dt_accuracy_mean, dt_accuracy_std, dt_auc_mean, dt_auc_std, dt_TPR_mean, dt_TPR_std, dt_FNR_mean, dt_FNR_std = cross_val(df_train, 'Decision Tree', dt, names[i])\n",
    "        rf_accuracy_mean, rf_accuracy_std, rf_auc_mean, rf_auc_std, rf_TPR_mean, rf_TPR_std,rf_FNR_mean, rf_FNR_std = cross_val(df_train, 'Random Forest', rf, names[i])\n",
    "        sgdc_accuracy_mean, sgdc_accuracy_std, sgdc_auc_mean, sgdc_auc_std, sgdc_TPR_mean, sgdc_TPR_std, sgdc_FNR_mean, sgdc_FNR_std = cross_val(df_train, 'SGDClassifier', sgdc, names[i])\n",
    "        gbc_accuracy_mean, gbc_accuracy_std, gbc_auc_mean, gbc_auc_std, gbc_TPR_mean, gbc_TPR_std,gbc_FNR_mean, gbc_FNR_std = cross_val(df_train, 'GradientBoostingClassifier', gbc, names[i])\n",
    "        f_csv.writerow([names[i],'',\n",
    "                        'lr_accuracy_mean','lr_auc_mean','lr_TPR_mean','lr_FNR_mean',\n",
    "                        'dt_accuracy_mean','dt_auc_mean','dt_TPR_mean','dt_FNR_mean',\n",
    "                        'rf_accuracy_mean','rf_auc_mean','rf_TPR_mean','rf_FNR_mean',\n",
    "                        'sgdc_accuracy_mean','sgdc_auc_mean','sgdc_TPR_mean','sgdc_FNR_mean',\n",
    "                        'gbc_accuracy_mean','gbc_auc_mean','gbc_TPR_mean','gbc_FNR_mean'])\n",
    "        f_csv.writerow(['','',lr_accuracy_mean,lr_auc_mean,lr_TPR_mean,lr_FNR_mean,\n",
    "                       dt_accuracy_mean,dt_auc_mean,dt_TPR_mean,dt_FNR_mean,\n",
    "                        rf_accuracy_mean,rf_auc_mean,rf_TPR_mean,rf_FNR_mean,\n",
    "                        sgdc_accuracy_mean,sgdc_auc_mean,sgdc_TPR_mean,sgdc_FNR_mean,\n",
    "                        gbc_accuracy_mean,gbc_auc_mean,gbc_TPR_mean,gbc_FNR_mean])\n",
    "        f_csv.writerow(['','','lr_accuracy_std','lr_auc_std','lr_TPR_std','lr_FNR_std',\n",
    "                        'dt_accuracy_std','dt_auc_std','dt_TPR_std','dt_FNR_std',\n",
    "                        'rf_accuracy_std','rf_auc_std','rf_TPR_std','rf_FNR_std',\n",
    "                        'sgdc_accuracy_std','sgdc_auc_std','sgdc_TPR_std','sgdc_FNR_std',\n",
    "                        'gbc_accuracy_std','gbc_auc_std','gbc_TPR_std','gbc_FNR_std'])\n",
    "        f_csv.writerow(['','',lr_accuracy_std,lr_auc_std,lr_TPR_std,lr_FNR_std,\n",
    "                       dt_accuracy_std,dt_auc_std,dt_TPR_std,dt_FNR_std,\n",
    "                        rf_accuracy_std,rf_auc_std,rf_TPR_std,rf_FNR_std,\n",
    "                        sgdc_accuracy_std,sgdc_auc_std,sgdc_TPR_std,sgdc_FNR_std,\n",
    "                        gbc_accuracy_std,gbc_auc_std,gbc_TPR_std,gbc_FNR_std])\n",
    "        print(\"The average auc of\",names[i],\":\", (lr_auc_mean+dt_auc_mean+rf_auc_mean+sgdc_auc_mean+gbc_auc_mean)/5)\n",
    "        print(\"--------------------------------------------------------\")\n",
    "    X_test = df_test[X_columns].values\n",
    "    y_test = df_test['readmitted'].values\n",
    "    print(\"Balance method: Ensemble different models\")\n",
    "    auc_mean,accuracy_mean,TPR_mean,FNR_mean = ensemble_predict([lr,dt,rf,sgdc,gbc],data_list,X_test,y_test)\n",
    "    f_csv.writerow([\"Ensemble\",'','accuracy_mean','auc_mean','TPR_mean','FNR_mean'])\n",
    "    f_csv.writerow(['','',accuracy_mean,auc_mean,TPR_mean,FNR_mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we use AUC as the criterion:\n",
    "\n",
    "The Bootstrapping method performs best. Logistic Regression performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_B = bootstrapping(df_train)    # Balance the whole train set\n",
    "X_train_B = train_B[X_columns].values\n",
    "y_train_B = train_B[\"readmitted\"].values\n",
    "X_train_tf,X_test_tf = scale_data(X_train_B,X_test)    # scale whole train set and untouched test\n",
    "lr.fit(X_train_tf, y_train_B)\n",
    "y_train_preds = lr.predict_proba(X_train_tf)[:,1]\n",
    "y_test_preds = lr.predict_proba(X_test_tf)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "auc:0.669 accuracy:0.619\n",
      "TPR:0.551 FNR:0.449\n",
      "recall:0.551\n",
      "precision:0.638\n",
      "specificity:0.687\n",
      "prevalence:0.500\n",
      "f1:0.592\n",
      "  \n",
      "Test:\n",
      "auc:0.665 accuracy:0.673\n",
      "TPR:0.543 FNR:0.457\n",
      "recall:0.543\n",
      "precision:0.184\n",
      "specificity:0.689\n",
      "prevalence:0.114\n",
      "f1:0.275\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print('Training:')\n",
    "auc, accuracy, TPR, FNR, recall, precision, specificity, prevalence, f1 = report(y_train_B, y_train_preds, thresh)\n",
    "print('auc:%.3f'%auc,'accuracy:%.3f'%accuracy)\n",
    "print('TPR:%.3f'%TPR,'FNR:%.3f'%FNR)\n",
    "print('recall:%.3f'%recall)\n",
    "print('precision:%.3f'%precision)\n",
    "print('specificity:%.3f'%specificity)\n",
    "print('prevalence:%.3f'%prevalence)\n",
    "print('f1:%.3f'%f1)\n",
    "print('  ')\n",
    "print('Test:')\n",
    "auc, accuracy, TPR, FNR, recall, precision, specificity, prevalence, f1 = report(y_test, y_test_preds, thresh)\n",
    "print('auc:%.3f'%auc,'accuracy:%.3f'%accuracy)\n",
    "print('TPR:%.3f'%TPR,'FNR:%.3f'%FNR)\n",
    "print('recall:%.3f'%recall)\n",
    "print('precision:%.3f'%precision)\n",
    "print('specificity:%.3f'%specificity)\n",
    "print('prevalence:%.3f'%prevalence)\n",
    "print('f1:%.3f'%f1)\n",
    "print('  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this project, we created a binary classifier to predict the probability that a patient with diabetes would be readmitted to the hospital within 30 days. On held out test data, our best model had an AUC of of 0.67. Using this model, we are able to catch 54% of the readmissions from our model that performs approximately 1.5 times better than randomly selecting patients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
